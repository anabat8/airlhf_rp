{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-15T17:06:43.905380Z",
     "start_time": "2024-06-15T17:06:14.416601Z"
    }
   },
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import torch \n",
    "import random\n",
    "import yaml\n",
    "\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from experiment3.RLHFAgent import RLHFAgent\n",
    "from experiment3.AIRLAgent import AIRLAgent\n",
    "from experiment3.Utils import Utils\n",
    "from experiment3.Environment import Environment"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T17:06:43.920901Z",
     "start_time": "2024-06-15T17:06:43.905380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_config(config_path, env_name):\n",
    "    with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config[env_name]"
   ],
   "id": "9f1b1f30c5db1ce8",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T17:07:20.848721Z",
     "start_time": "2024-06-15T17:07:20.817475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config_path = \"parameters.yaml\"\n",
    "env_name = \"cartpole\"  # or \"pendulum\"\n",
    "config_rlhf = load_config(config_path, env_name + \"RLHF\")\n",
    "config_airl = load_config(config_path, env_name + \"AIRL\")"
   ],
   "id": "b03afdd3fa34adb8",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T17:08:07.571106Z",
     "start_time": "2024-06-15T17:08:07.539493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize environment\n",
    "SEED = 42\n",
    "if env_name == \"cartpole\":\n",
    "    env = Environment(\"seals:seals/CartPole-v0\", SEED, num_envs=8)\n",
    "elif env_name == \"pendulum\":\n",
    "    env = Environment(\"Pendulum-v1\", SEED, num_envs=4)\n",
    "    \n",
    "env.init_vec_env()\n",
    "print(env.env_id)"
   ],
   "id": "dffee0d2ba446207",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seals:seals/CartPole-v0\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training scripts",
   "id": "f8ddb06b99e01e84"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T17:08:10.225257Z",
     "start_time": "2024-06-15T17:08:10.210125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_rlhf(num_comparisons, exploration_frac=0.05, fragment_length=100):\n",
    "    # Initialize RLHFAgent\n",
    "    rlhfAgent = RLHFAgent(env_object=env)\n",
    "    rlhfAgent.init_gen_algo(config=config_rlhf, ac_policy=MlpPolicy, env_object=env)\n",
    "    rlhfAgent.init_trajectory_gen(env_object=env, exploration_frac=exploration_frac)\n",
    "    \n",
    "    # Train RLHFAgent alone\n",
    "    rlhfAgent.train(save_path=pathlib.Path(\"rlhf_agent\"), env_object=env, total_human_comparisons=num_comparisons,    total_timesteps=400_000, fragment_length=fragment_length)\n"
   ],
   "id": "373ad432019d8ff1",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T17:08:12.220868Z",
     "start_time": "2024-06-15T17:08:12.205238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_airl(expert_type, nr_demonstrations, random_prob=0, switch_prob=0):\n",
    "    # Initialize AIRLAgent\n",
    "    # To introduce suboptimality in expert demonstrations, specify random_prob and switch_prob\n",
    "    airlAgent = AIRLAgent(env_object=env, expert_type=expert_type, nr_demonstrations=nr_demonstrations,\n",
    "                          random_prob=random_prob, switch_prob=switch_prob)\n",
    "    airlAgent.init_gen_algo(config=config_airl, ac_policy=MlpPolicy, env_object=env)\n",
    "    \n",
    "    # Train AIRLAgent alone\n",
    "    airlAgent.train(env_object=env, train_steps=400_000)"
   ],
   "id": "ee34b7d661c87f12",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T17:08:12.948944Z",
     "start_time": "2024-06-15T17:08:12.932921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_irlhf(reward_net_airl, num_comparisons, exploration_frac=0.05, fragment_length=100, num_it=60, initial_epoch_multiplier=4):\n",
    "    # Initialize RLHFAgent\n",
    "    rlhfAgent = RLHFAgent(env_object=env)\n",
    "    rlhfAgent.set_reward_from_airl(reward_net_airl, env_object=env)\n",
    "    # To only pass reward, remove path_to_algo parameter / or set to None\n",
    "    # We are passing both reward_airl and policy_airl\n",
    "    rlhfAgent.init_gen_algo(config=config_rlhf, ac_policy=MlpPolicy, env_object=env, path_to_algo=\"airl_agent/gen_policy/model.zip\")\n",
    "    rlhfAgent.init_trajectory_gen(env_object=env, exploration_frac=exploration_frac)\n",
    "    \n",
    "    # Train RLHFAgent with reward and policy initialized by AIRL\n",
    "    rlhfAgent.train(save_path=pathlib.Path(\"irlhf_agent\"), env_object=env, total_human_comparisons=num_comparisons, total_timesteps=400_000,\n",
    "     fragment_length=fragment_length, num_it=num_it, initial_epoch_multiplier=initial_epoch_multiplier)\n",
    "    "
   ],
   "id": "425bfdb0a84a8496",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T17:08:14.138249Z",
     "start_time": "2024-06-15T17:08:14.106618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_reward_stats(path):\n",
    "    rewards = torch.load(path)\n",
    "    return rewards, np.mean(rewards), np.std(rewards)"
   ],
   "id": "c0ba932aa6bc8650",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T17:08:14.449151Z",
     "start_time": "2024-06-15T17:08:14.433137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.policies.base import NormalizeFeaturesExtractor\n",
    "\n",
    "\n",
    "def train_with_learned_reward_and_evaluate(reward_path, train_path, tensorboard_dir, tb_log_name, \n",
    "                                           wandb_project_name, wandb_save_path, config\n",
    "                                           ):\n",
    "    reward_net = torch.load(reward_path)\n",
    "    Utils.train_with_learned_reward(learned_reward=reward_net, save_path=train_path, config=config, \n",
    "                                    ac_policy=MlpPolicy, tensorboard_dir=tensorboard_dir, tb_log_name=tb_log_name, \n",
    "                                    env_object=env, wandb_project_name=wandb_project_name, wandb_save_path=wandb_save_path,\n",
    "                                    policy_kwargs=dict(\n",
    "                                        features_extractor_class=NormalizeFeaturesExtractor,\n",
    "                                        features_extractor_kwargs=dict(normalize_class=RunningNorm),\n",
    "                                    ))\n",
    "    eval_mean, eval_std = Utils.evaluate_trained_agent_with_learned_reward(load_path=train_path, venv=env.venv)\n",
    "    return eval_mean, eval_std"
   ],
   "id": "e98e428e75e819b2",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T17:08:15.350472Z",
     "start_time": "2024-06-15T17:08:15.338445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # wrap env in new seed\n",
    "    env.seed = seed \n",
    "    env.rng = np.random.default_rng(seed)\n",
    "    env.init_vec_env()\n",
    "    env.venv.seed(seed)"
   ],
   "id": "21d3cd0b107948f6",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training with different seeds and different demonstrations AIRL agent",
   "id": "798719038ed967c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T17:09:48.989314Z",
     "start_time": "2024-06-15T17:09:48.979312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main_train_airl(expert_type, nr_demonstrations, random_prob=0, switch_prob=0):\n",
    "    rewards_over_seeds = [] \n",
    "    seed_list = [34, 43, 52, 61, 70]\n",
    "    \n",
    "    for i, seed in enumerate(seed_list): \n",
    "        # set seed\n",
    "        set_seed(seed)\n",
    "        \n",
    "        # start airl\n",
    "        means = []\n",
    "        stds = []\n",
    "        eval_policy_mean = []\n",
    "        eval_policy_std = []\n",
    "        \n",
    "        # Train\n",
    "        train_airl(expert_type=expert_type, nr_demonstrations=nr_demonstrations, random_prob=random_prob, switch_prob=switch_prob)\n",
    "        \n",
    "        rws, mean, std = get_reward_stats(\"./airl_agent/learner_rewards.pt\")\n",
    "            \n",
    "        means.append(mean)\n",
    "        stds.append(std)\n",
    "                    \n",
    "        eval_mean, eval_std = train_with_learned_reward_and_evaluate(\"airl_agent/reward_net.pt\", \"airl_agent/airl_agent_trained_with_learned_reward\",  \"./airl_cartpole_tensorboard\", f\"run_4_expert_with_{seed}\", \n",
    "         wandb_project_name=\"airl\", wandb_save_path=f\"models/run_4_expert_with_{seed}\", \n",
    "         config=config_airl\n",
    "         )\n",
    "                \n",
    "        eval_policy_mean.append(eval_mean)\n",
    "        eval_policy_std.append(eval_std)\n",
    "            \n",
    "        # print(f\"number of expert demonstrations = {expert_dem}: mean={mean}, std={std}\")\n",
    "        # print(f\"number of expert demonstrations = {expert_dem}: eval_mean={eval_mean}, eval_std={eval_std}\")\n",
    "            \n",
    "        rewards_over_seeds.append(eval_policy_mean)"
   ],
   "id": "dc5a0590fd46a627",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "main_train_airl(\"suboptimal\", 60, random_prob=0.25, switch_prob=0.2)",
   "id": "7d28b9f159d921ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training with different seeds and different nr of queries RLHF / IRLHF",
   "id": "56e77d98f7327ce8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T09:38:11.122696Z",
     "start_time": "2024-06-15T09:38:11.107040Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main_train(agent_name, path, expert_type, nr_demonstrations, random_prob=0, switch_prob=0):\n",
    "    rewards_over_seeds = [] # [0]: lists of means for seed 0, num comp 250,500,600 respectively\n",
    "    seed_list = [34, 43, 52, 61, 70]\n",
    "    comparisons_list = [1400]\n",
    "    \n",
    "    for i, seed in enumerate(seed_list): \n",
    "        # set seed\n",
    "        set_seed(seed)\n",
    "        \n",
    "        # start rlhf / irlhf \n",
    "        means = []\n",
    "        stds = []\n",
    "        eval_policy_mean = []\n",
    "        eval_policy_std = []\n",
    "    \n",
    "        for idx, num_comparisons in enumerate(comparisons_list):\n",
    "            if agent_name == \"rlhf\":\n",
    "                train_rlhf(num_comparisons)\n",
    "            if agent_name == \"irlhf\":\n",
    "                # Train airl under current seed\n",
    "                train_airl(expert_type=expert_type, nr_demonstrations=nr_demonstrations,\n",
    "                           random_prob=random_prob, switch_prob=switch_prob)\n",
    "                # Load trained AIRLAgent reward function\n",
    "                reward_net_airl = torch.load(\"airl_agent/reward_net.pt\")\n",
    "                # Pass reward_net_airl to irlhf\n",
    "                # Train irlhf (optimize reward function)\n",
    "                if num_comparisons == 10:\n",
    "                    train_irlhf(reward_net_airl, num_comparisons, num_it=8)\n",
    "                else:\n",
    "                    train_irlhf(reward_net_airl, num_comparisons)\n",
    "                \n",
    "            rws, mean, std = get_reward_stats(path)\n",
    "            \n",
    "            means.append(mean)\n",
    "            stds.append(std)\n",
    "            \n",
    "            eval_mean = None\n",
    "            eval_std = None\n",
    "            \n",
    "            # Train agent using the learned reward, optimize policy\n",
    "            \n",
    "            if agent_name == \"rlhf\":\n",
    "                eval_mean, eval_std = train_with_learned_reward_and_evaluate(\n",
    "                    reward_path=\"rlhf_agent/reward_net.pt\",\n",
    "                    train_path=\"rlhf_agent/rlhf_agent_trained_with_learned_reward\",\n",
    "                    tensorboard_dir=\"./ppo_rlhf_cartpole_tensorboard/\",\n",
    "                    tb_log_name=f\"run5_comparisons_{num_comparisons}_with_seed_{seed}\",\n",
    "                    wandb_project_name=\"rlhf\",\n",
    "                    wandb_save_path=f\"models/run_comparisons_{num_comparisons}_with_seed_{seed}\",\n",
    "                    config=config_rlhf\n",
    "                )\n",
    "            if agent_name == \"irlhf\":\n",
    "                eval_mean, eval_std = train_with_learned_reward_and_evaluate(\n",
    "                    reward_path=\"irlhf_agent/reward_net.pt\",\n",
    "                    train_path=\"irlhf_agent/irlhf_agent_trained_with_learned_reward\",\n",
    "                    tensorboard_dir=\"./ppo_irlhf_cartpole_tensorboard_3/\",\n",
    "                    tb_log_name=f\"run_irlhf_comparisons_{num_comparisons}_demonstrations_{nr_demonstrations}_with_seed_{seed}_with_{expert_type}_expert\",\n",
    "                    wandb_project_name=\"irlhf\",\n",
    "                    wandb_save_path=f\"models/run_comparisons_{num_comparisons}_with_seed_{seed}_irlhf\",\n",
    "                    config=config_rlhf\n",
    "                )\n",
    "                \n",
    "            eval_policy_mean.append(eval_mean)\n",
    "            eval_policy_std.append(eval_std)\n",
    "            \n",
    "            print(f\"number of comparisons = {num_comparisons}: mean={mean}, std={std}\")\n",
    "            print(f\"number of comparisons = {num_comparisons}: eval_mean={eval_mean}, eval_std={eval_std}\")\n",
    "            \n",
    "        rewards_over_seeds.append(eval_policy_mean)"
   ],
   "id": "ef1029fef7a06040",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T09:38:13.190476Z",
     "start_time": "2024-06-15T09:38:13.174840Z"
    }
   },
   "cell_type": "code",
   "source": "# main_train(\"rlhf\", \"./rlhf_agent/learner_rewards.pt\")",
   "id": "8badcedc2672e530",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for demonstrations in [1400]:\n",
    "    main_train(\"irlhf\", \"./irlhf_agent/learner_rewards.pt\", expert_type=\"suboptimal\", nr_demonstrations=demonstrations,\n",
    "               random_prob=0.25,\n",
    "               switch_prob=0.2)"
   ],
   "id": "c02575f6ea72e9ad",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
