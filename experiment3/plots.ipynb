{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-23T16:21:44.449186Z",
     "start_time": "2024-06-23T16:21:44.423141Z"
    }
   },
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import torch \n",
    "import random\n",
    "import yaml\n",
    "\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from experiment3.RLHFAgent import RLHFAgent\n",
    "from experiment3.AIRLAgent import AIRLAgent\n",
    "from experiment3.Utils import Utils\n",
    "from experiment3.Environment import Environment"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T16:21:45.258724Z",
     "start_time": "2024-06-23T16:21:45.243126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_config(config_path, env_name):\n",
    "    with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config[env_name]"
   ],
   "id": "9f1b1f30c5db1ce8",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T16:21:45.977834Z",
     "start_time": "2024-06-23T16:21:45.907840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config_path = \"parameters.yaml\"\n",
    "env_name = \"pendulum\" # or \"cartpole\" \n",
    "config_rlhf = load_config(config_path, env_name + \"RLHF\")\n",
    "config_airl = load_config(config_path, env_name + \"AIRL\")\n",
    "\n",
    "print(config_rlhf)\n",
    "print(config_airl)"
   ],
   "id": "b03afdd3fa34adb8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'policy_name': 'ppo', 'learning_rate': 0.001, 'n_steps': 1024, 'batch_size': 64, 'n_epochs': 10, 'gae_lambda': 0.95, 'gamma': 0.9, 'clip_range': 0.2, 'ent_coef': 0.0, 'vf_coef': 0.5, 'num_envs': 4}\n",
      "{'policy_name': 'ppo', 'learning_rate': 0.001, 'n_steps': 1024, 'batch_size': 64, 'n_epochs': 10, 'gae_lambda': 0.95, 'gamma': 0.9, 'clip_range': 0.2, 'ent_coef': 0.0, 'vf_coef': 0.5, 'num_envs': 4}\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T16:21:47.672609Z",
     "start_time": "2024-06-23T16:21:47.597298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize environment\n",
    "SEED = 42\n",
    "if env_name == \"cartpole\":\n",
    "    env = Environment(\"seals:seals/CartPole-v0\", SEED, num_envs=8)\n",
    "elif env_name == \"pendulum\":\n",
    "    env = Environment(\"Pendulum-v1\", SEED, num_envs=4)\n",
    "    \n",
    "env.init_vec_env()\n",
    "env.init_test_vec_env()\n",
    "print(env.env_id)"
   ],
   "id": "dffee0d2ba446207",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pendulum-v1\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training scripts",
   "id": "f8ddb06b99e01e84"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T16:21:49.109781Z",
     "start_time": "2024-06-23T16:21:49.078338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_rlhf(num_comparisons, exploration_frac=0.05, fragment_length=100):\n",
    "    # Initialize RLHFAgent\n",
    "    rlhfAgent = RLHFAgent(env_object=env)\n",
    "    rlhfAgent.init_gen_algo(config=config_rlhf, ac_policy=MlpPolicy, env_object=env)\n",
    "    rlhfAgent.init_trajectory_gen(env_object=env, exploration_frac=exploration_frac)\n",
    "    \n",
    "    # Train RLHFAgent alone\n",
    "    rlhfAgent.train(save_path=pathlib.Path(\"rlhf_agent\"), env_object=env, total_human_comparisons=num_comparisons, total_timesteps=400_000, fragment_length=fragment_length)\n"
   ],
   "id": "373ad432019d8ff1",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T16:21:49.912422Z",
     "start_time": "2024-06-23T16:21:49.880797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_airl(expert_type, nr_demonstrations, random_prob=0, switch_prob=0):\n",
    "    # Initialize AIRLAgent\n",
    "    # To introduce suboptimality in expert demonstrations, specify random_prob and switch_prob\n",
    "    airlAgent = AIRLAgent(env_object=env, expert_type=expert_type, nr_demonstrations=nr_demonstrations,\n",
    "                          random_prob=random_prob, switch_prob=switch_prob)\n",
    "    airlAgent.init_gen_algo(config=config_airl, ac_policy=MlpPolicy, env_object=env)\n",
    "    \n",
    "    # Train AIRLAgent alone\n",
    "    airlAgent.train(env_object=env, train_steps=400_000)"
   ],
   "id": "ee34b7d661c87f12",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T16:21:50.470924Z",
     "start_time": "2024-06-23T16:21:50.439283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_irlhf(reward_net_airl, num_comparisons, exploration_frac=0.05, fragment_length=100, num_it=60, initial_epoch_multiplier=4):\n",
    "    # Initialize RLHFAgent\n",
    "    rlhfAgent = RLHFAgent(env_object=env)\n",
    "    rlhfAgent.set_reward_from_airl(reward_net_airl, env_object=env)\n",
    "    # To only pass reward, remove path_to_algo parameter / or set to None\n",
    "    # We are passing both reward_airl and policy_airl\n",
    "    rlhfAgent.init_gen_algo(config=config_rlhf, ac_policy=MlpPolicy, env_object=env, path_to_algo=\"airl_agent/gen_policy/model.zip\")\n",
    "    rlhfAgent.init_trajectory_gen(env_object=env, exploration_frac=exploration_frac)\n",
    "    \n",
    "    # Train RLHFAgent with reward and policy initialized by AIRL\n",
    "    rlhfAgent.train(save_path=pathlib.Path(\"irlhf_agent\"), env_object=env, total_human_comparisons=num_comparisons, total_timesteps=400_000,\n",
    "     fragment_length=fragment_length, num_it=num_it, initial_epoch_multiplier=initial_epoch_multiplier)\n",
    "    "
   ],
   "id": "425bfdb0a84a8496",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T16:21:51.161069Z",
     "start_time": "2024-06-23T16:21:51.148306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_reward_stats(path):\n",
    "    rewards = torch.load(path)\n",
    "    return rewards, np.mean(rewards), np.std(rewards)"
   ],
   "id": "c0ba932aa6bc8650",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T16:21:51.698975Z",
     "start_time": "2024-06-23T16:21:51.678980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.policies.base import NormalizeFeaturesExtractor\n",
    "\n",
    "\n",
    "def train_with_learned_reward_and_evaluate(reward_path, train_path, tensorboard_dir, tb_log_name, \n",
    "                                           config\n",
    "                                           ):\n",
    "    reward_net = torch.load(reward_path)\n",
    "    Utils.train_with_learned_reward(learned_reward=reward_net, save_path=train_path, config=config, \n",
    "                                    ac_policy=MlpPolicy, tensorboard_dir=tensorboard_dir, tb_log_name=tb_log_name, \n",
    "                                    env_object=env, \n",
    "                                    policy_kwargs=dict(\n",
    "                                        features_extractor_class=NormalizeFeaturesExtractor,\n",
    "                                        features_extractor_kwargs=dict(normalize_class=RunningNorm),\n",
    "                                    ))\n",
    "    \n",
    "    # Evaluate with the true reward by using the unwrapped test vector environment\n",
    "    eval_mean, eval_std = Utils.evaluate_trained_agent_with_true_reward(load_path=train_path, venv=env.test_venv)\n",
    "    return eval_mean, eval_std"
   ],
   "id": "e98e428e75e819b2",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T16:21:52.498347Z",
     "start_time": "2024-06-23T16:21:52.485456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # wrap env in new seed\n",
    "    env.seed = seed \n",
    "    env.rng = np.random.default_rng(seed)\n",
    "    env.init_vec_env()\n",
    "    env.init_test_vec_env()\n",
    "    env.venv.seed(seed)\n",
    "    env.test_venv.seed(seed)"
   ],
   "id": "21d3cd0b107948f6",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training with different seeds and different demonstrations AIRL agent",
   "id": "798719038ed967c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T16:21:54.655326Z",
     "start_time": "2024-06-23T16:21:54.635215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main_train_airl(expert_type, nr_demonstrations, random_prob=0, switch_prob=0):\n",
    "    rewards_over_seeds = [] \n",
    "    seed_list = [34, 43, 52, 61, 70]\n",
    "    \n",
    "    for i, seed in enumerate(seed_list): \n",
    "        # set seed\n",
    "        set_seed(seed)\n",
    "        \n",
    "        # start airl\n",
    "        means = []\n",
    "        stds = []\n",
    "        eval_policy_mean = []\n",
    "        eval_policy_std = []\n",
    "        \n",
    "        # Train\n",
    "        train_airl(expert_type=expert_type, nr_demonstrations=nr_demonstrations, random_prob=random_prob, switch_prob=switch_prob)\n",
    "        \n",
    "        rws, mean, std = get_reward_stats(\"./airl_agent/learner_rewards.pt\")\n",
    "            \n",
    "        means.append(mean)\n",
    "        stds.append(std)\n",
    "                    \n",
    "        eval_mean, eval_std = train_with_learned_reward_and_evaluate(\"airl_agent/reward_net.pt\", \"airl_agent/airl_agent_trained_with_learned_reward\",  f\"./airl_{env_name}_tensorboard\", f\"run_expert_with_{seed}_dem_{nr_demonstrations}_{env_name}\", \n",
    "         config=config_airl\n",
    "         )\n",
    "                \n",
    "        eval_policy_mean.append(eval_mean)\n",
    "        eval_policy_std.append(eval_std)\n",
    "            \n",
    "        # print(f\"number of expert demonstrations = {expert_dem}: mean={mean}, std={std}\")\n",
    "        # print(f\"number of expert demonstrations = {expert_dem}: eval_mean={eval_mean}, eval_std={eval_std}\")\n",
    "            \n",
    "        rewards_over_seeds.append(eval_policy_mean)"
   ],
   "id": "dc5a0590fd46a627",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# main_train_airl(\"suboptimal\", 60, random_prob=0.25, switch_prob=0.2)\n",
    "main_train_airl(\"optimal\", 1400)"
   ],
   "id": "7d28b9f159d921ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training with different seeds and different nr of queries RLHF / IRLHF",
   "id": "56e77d98f7327ce8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T10:08:41.798054Z",
     "start_time": "2024-06-23T10:08:41.784340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "eval_mean_over_seeds = [] # [0]: lists of means for seed 0, num comp 250,500,600 respectively\n",
    "eval_std_over_seeds = [] \n",
    "\n",
    "def main_train(agent_name, path, expert_type, nr_demonstrations, random_prob=0, switch_prob=0):\n",
    "    seed_list = [34, 43, 52, 61, 70]\n",
    "    comparisons_list = [800]\n",
    "    \n",
    "    for i, seed in enumerate(seed_list): \n",
    "        # set seed\n",
    "        set_seed(seed)\n",
    "        \n",
    "        # start rlhf / irlhf \n",
    "        means = []\n",
    "        stds = []\n",
    "        eval_policy_mean = []\n",
    "        eval_policy_std = []\n",
    "        \n",
    "        for idx, num_comparisons in enumerate(comparisons_list):\n",
    "            if agent_name == \"rlhf\":\n",
    "                train_rlhf(num_comparisons)\n",
    "            if agent_name == \"irlhf\":\n",
    "                # Train airl under current seed\n",
    "                train_airl(expert_type=expert_type, nr_demonstrations=nr_demonstrations,\n",
    "                           random_prob=random_prob, switch_prob=switch_prob)\n",
    "                # Load trained AIRLAgent reward function\n",
    "                reward_net_airl = torch.load(\"airl_agent/reward_net.pt\")\n",
    "                # Pass reward_net_airl to irlhf\n",
    "                # Train irlhf (optimize reward function)\n",
    "                if num_comparisons == 10:\n",
    "                    train_irlhf(reward_net_airl, num_comparisons, num_it=8)\n",
    "                else:\n",
    "                    train_irlhf(reward_net_airl, num_comparisons)\n",
    "            \n",
    "            # Stats after learning reward function\n",
    "            rws, mean, std = get_reward_stats(path) \n",
    "            \n",
    "            means.append(mean)\n",
    "            stds.append(std)\n",
    "            \n",
    "            eval_mean = None\n",
    "            eval_std = None\n",
    "            \n",
    "            # Train agent using the learned reward, optimize policy\n",
    "            \n",
    "            if agent_name == \"rlhf\":\n",
    "                eval_mean, eval_std = train_with_learned_reward_and_evaluate(\n",
    "                    reward_path=\"rlhf_agent/reward_net.pt\",\n",
    "                    train_path=\"rlhf_agent/rlhf_agent_trained_with_learned_reward\",\n",
    "                    tensorboard_dir=f\"./ppo_rlhf_{env_name}_tensorboard_4/\",\n",
    "                    tb_log_name=f\"run_comparisons_{num_comparisons}_with_seed_{seed}_{env_name}\",\n",
    "                    config=config_rlhf\n",
    "                )\n",
    "            if agent_name == \"irlhf\":\n",
    "                eval_mean, eval_std = train_with_learned_reward_and_evaluate(\n",
    "                    reward_path=\"irlhf_agent/reward_net.pt\",\n",
    "                    train_path=\"irlhf_agent/irlhf_agent_trained_with_learned_reward\",\n",
    "                    tensorboard_dir=f\"./ppo_irlhf_{env_name}_tensorboard_4/\",\n",
    "                    tb_log_name=f\"run_irlhf_comparisons_{num_comparisons}_demonstrations_{nr_demonstrations}_with_seed_{seed}_with_{expert_type}_expert\",\n",
    "                    config=config_rlhf\n",
    "                )\n",
    "                \n",
    "            eval_policy_mean.append(eval_mean)\n",
    "            eval_policy_std.append(eval_std)\n",
    "            \n",
    "            print(f\"number of comparisons = {num_comparisons}: mean={mean}, std={std}\")\n",
    "            print(f\"number of comparisons = {num_comparisons}: eval_mean={eval_mean}, eval_std={eval_std}\")\n",
    "        \n",
    "        # Append evaluation results for policy under true reward\n",
    "        eval_mean_over_seeds.append(eval_policy_mean)   # for every seed, all eval means for all comparisons are concatenated\n",
    "        eval_std_over_seeds.append(eval_policy_std)"
   ],
   "id": "ef1029fef7a06040",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T10:08:43.222326Z",
     "start_time": "2024-06-23T10:08:43.215827Z"
    }
   },
   "cell_type": "code",
   "source": "main_train(\"rlhf\", \"./rlhf_agent/learner_rewards.pt\", expert_type=\"optimal\", nr_demonstrations=60)",
   "id": "8badcedc2672e530",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(eval_mean_over_seeds)\n",
    "print(eval_std_over_seeds)"
   ],
   "id": "2dbf624855d84c0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for demonstrations in [1000]:\n",
    "    main_train(\"irlhf\", \"./irlhf_agent/learner_rewards.pt\", expert_type=\"optimal\", nr_demonstrations=demonstrations)\n",
    "                # random_prob=0.10,\n",
    "                # switch_prob=0.20)"
   ],
   "id": "c02575f6ea72e9ad",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
