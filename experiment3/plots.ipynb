{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-20T09:52:55.085344Z",
     "start_time": "2024-06-20T09:52:23.287223Z"
    }
   },
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import torch \n",
    "import random\n",
    "import yaml\n",
    "\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from experiment3.RLHFAgent import RLHFAgent\n",
    "from experiment3.AIRLAgent import AIRLAgent\n",
    "from experiment3.Utils import Utils\n",
    "from experiment3.Environment import Environment"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T09:52:58.741423Z",
     "start_time": "2024-06-20T09:52:58.725800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_config(config_path, env_name):\n",
    "    with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config[env_name]"
   ],
   "id": "9f1b1f30c5db1ce8",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T09:52:59.397445Z",
     "start_time": "2024-06-20T09:52:59.365774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config_path = \"parameters.yaml\"\n",
    "env_name = \"cartpole\" # or \"pendulum\"\n",
    "config_rlhf = load_config(config_path, env_name + \"RLHF\")\n",
    "config_airl = load_config(config_path, env_name + \"AIRL\")\n",
    "\n",
    "print(config_rlhf)\n",
    "print(config_airl)"
   ],
   "id": "b03afdd3fa34adb8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'policy_name': 'ppo', 'learning_rate': 0.001, 'n_steps': 32, 'batch_size': 64, 'n_epochs': 20, 'gae_lambda': 0.8, 'gamma': 0.98, 'clip_range': 0.2, 'ent_coef': 0.0, 'vf_coef': 0.1, 'num_envs': 8}\n",
      "{'policy_name': 'ppo', 'learning_rate': 0.0005, 'n_steps': 2048, 'batch_size': 64, 'n_epochs': 5, 'gae_lambda': 0.95, 'gamma': 0.95, 'clip_range': 0.1, 'ent_coef': 0.0, 'vf_coef': 0.1, 'num_envs': 8}\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T09:53:03.160913Z",
     "start_time": "2024-06-20T09:53:03.023116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize environment\n",
    "SEED = 42\n",
    "if env_name == \"cartpole\":\n",
    "    env = Environment(\"seals:seals/CartPole-v0\", SEED, num_envs=8)\n",
    "elif env_name == \"pendulum\":\n",
    "    env = Environment(\"Pendulum-v1\", SEED, num_envs=4)\n",
    "    \n",
    "env.init_vec_env()\n",
    "env.init_test_vec_env()\n",
    "print(env.env_id)"
   ],
   "id": "dffee0d2ba446207",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seals:seals/CartPole-v0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ana\\PycharmProjects\\research_project\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:233: UserWarning: Starting from gymnasium v0.26, render modes are determined during the initialization of the environment.\n",
      "                We allow to pass a mode argument to maintain a backwards compatible VecEnv API, but the mode (rgb_array)\n",
      "                has to be the same as the environment render mode (None) which is not the case.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training scripts",
   "id": "f8ddb06b99e01e84"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T09:53:05.623899Z",
     "start_time": "2024-06-20T09:53:05.608233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_rlhf(num_comparisons, exploration_frac=0.05, fragment_length=100):\n",
    "    # Initialize RLHFAgent\n",
    "    rlhfAgent = RLHFAgent(env_object=env)\n",
    "    rlhfAgent.init_gen_algo(config=config_rlhf, ac_policy=MlpPolicy, env_object=env)\n",
    "    rlhfAgent.init_trajectory_gen(env_object=env, exploration_frac=exploration_frac)\n",
    "    \n",
    "    # Train RLHFAgent alone\n",
    "    rlhfAgent.train(save_path=pathlib.Path(\"rlhf_agent\"), env_object=env, total_human_comparisons=num_comparisons, total_timesteps=400_000, fragment_length=fragment_length)\n"
   ],
   "id": "373ad432019d8ff1",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T09:53:06.285111Z",
     "start_time": "2024-06-20T09:53:06.269488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_airl(expert_type, nr_demonstrations, random_prob=0, switch_prob=0):\n",
    "    # Initialize AIRLAgent\n",
    "    # To introduce suboptimality in expert demonstrations, specify random_prob and switch_prob\n",
    "    airlAgent = AIRLAgent(env_object=env, expert_type=expert_type, nr_demonstrations=nr_demonstrations,\n",
    "                          random_prob=random_prob, switch_prob=switch_prob)\n",
    "    airlAgent.init_gen_algo(config=config_airl, ac_policy=MlpPolicy, env_object=env)\n",
    "    \n",
    "    # Train AIRLAgent alone\n",
    "    airlAgent.train(env_object=env, train_steps=400_000)"
   ],
   "id": "ee34b7d661c87f12",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T09:53:06.767750Z",
     "start_time": "2024-06-20T09:53:06.736031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_irlhf(reward_net_airl, num_comparisons, exploration_frac=0.05, fragment_length=100, num_it=60, initial_epoch_multiplier=4):\n",
    "    # Initialize RLHFAgent\n",
    "    rlhfAgent = RLHFAgent(env_object=env)\n",
    "    rlhfAgent.set_reward_from_airl(reward_net_airl, env_object=env)\n",
    "    # To only pass reward, remove path_to_algo parameter / or set to None\n",
    "    # We are passing both reward_airl and policy_airl\n",
    "    rlhfAgent.init_gen_algo(config=config_rlhf, ac_policy=MlpPolicy, env_object=env, path_to_algo=\"airl_agent/gen_policy/model.zip\")\n",
    "    rlhfAgent.init_trajectory_gen(env_object=env, exploration_frac=exploration_frac)\n",
    "    \n",
    "    # Train RLHFAgent with reward and policy initialized by AIRL\n",
    "    rlhfAgent.train(save_path=pathlib.Path(\"irlhf_agent\"), env_object=env, total_human_comparisons=num_comparisons, total_timesteps=400_000,\n",
    "     fragment_length=fragment_length, num_it=num_it, initial_epoch_multiplier=initial_epoch_multiplier)\n",
    "    "
   ],
   "id": "425bfdb0a84a8496",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T09:53:07.492139Z",
     "start_time": "2024-06-20T09:53:07.476201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_reward_stats(path):\n",
    "    rewards = torch.load(path)\n",
    "    return rewards, np.mean(rewards), np.std(rewards)"
   ],
   "id": "c0ba932aa6bc8650",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T09:53:07.914157Z",
     "start_time": "2024-06-20T09:53:07.898432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.policies.base import NormalizeFeaturesExtractor\n",
    "\n",
    "\n",
    "def train_with_learned_reward_and_evaluate(reward_path, train_path, tensorboard_dir, tb_log_name, \n",
    "                                           wandb_project_name, wandb_save_path, config\n",
    "                                           ):\n",
    "    reward_net = torch.load(reward_path)\n",
    "    Utils.train_with_learned_reward(learned_reward=reward_net, save_path=train_path, config=config, \n",
    "                                    ac_policy=MlpPolicy, tensorboard_dir=tensorboard_dir, tb_log_name=tb_log_name, \n",
    "                                    env_object=env, wandb_project_name=wandb_project_name, wandb_save_path=wandb_save_path,\n",
    "                                    policy_kwargs=dict(\n",
    "                                        features_extractor_class=NormalizeFeaturesExtractor,\n",
    "                                        features_extractor_kwargs=dict(normalize_class=RunningNorm),\n",
    "                                    ))\n",
    "    \n",
    "    # Evaluate with the true reward by using the unwrapped test vector environment\n",
    "    eval_mean, eval_std = Utils.evaluate_trained_agent_with_true_reward(load_path=train_path, venv=env.test_venv)\n",
    "    return eval_mean, eval_std"
   ],
   "id": "e98e428e75e819b2",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T09:53:08.616093Z",
     "start_time": "2024-06-20T09:53:08.600146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # wrap env in new seed\n",
    "    env.seed = seed \n",
    "    env.rng = np.random.default_rng(seed)\n",
    "    env.init_vec_env()\n",
    "    env.init_test_vec_env()\n",
    "    env.venv.seed(seed)\n",
    "    env.test_venv.seed(seed)"
   ],
   "id": "21d3cd0b107948f6",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training with different seeds and different demonstrations AIRL agent",
   "id": "798719038ed967c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T23:12:44.031920Z",
     "start_time": "2024-06-19T23:12:44.016256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main_train_airl(expert_type, nr_demonstrations, random_prob=0, switch_prob=0):\n",
    "    rewards_over_seeds = [] \n",
    "    seed_list = [34, 43, 52, 61, 70]\n",
    "    \n",
    "    for i, seed in enumerate(seed_list): \n",
    "        # set seed\n",
    "        set_seed(seed)\n",
    "        \n",
    "        # start airl\n",
    "        means = []\n",
    "        stds = []\n",
    "        eval_policy_mean = []\n",
    "        eval_policy_std = []\n",
    "        \n",
    "        # Train\n",
    "        train_airl(expert_type=expert_type, nr_demonstrations=nr_demonstrations, random_prob=random_prob, switch_prob=switch_prob)\n",
    "        \n",
    "        rws, mean, std = get_reward_stats(\"./airl_agent/learner_rewards.pt\")\n",
    "            \n",
    "        means.append(mean)\n",
    "        stds.append(std)\n",
    "                    \n",
    "        eval_mean, eval_std = train_with_learned_reward_and_evaluate(\"airl_agent/reward_net.pt\", \"airl_agent/airl_agent_trained_with_learned_reward\",  f\"./airl_{env_name}_tensorboard\", f\"run_expert_with_{seed}_dem_{nr_demonstrations}_{env_name}\", \n",
    "         wandb_project_name=\"airl\", wandb_save_path=f\"models/run_4_expert_with_{seed}\", \n",
    "         config=config_airl\n",
    "         )\n",
    "                \n",
    "        eval_policy_mean.append(eval_mean)\n",
    "        eval_policy_std.append(eval_std)\n",
    "            \n",
    "        # print(f\"number of expert demonstrations = {expert_dem}: mean={mean}, std={std}\")\n",
    "        # print(f\"number of expert demonstrations = {expert_dem}: eval_mean={eval_mean}, eval_std={eval_std}\")\n",
    "            \n",
    "        rewards_over_seeds.append(eval_policy_mean)"
   ],
   "id": "dc5a0590fd46a627",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# main_train_airl(\"suboptimal\", 60, random_prob=0.25, switch_prob=0.2)\n",
    "main_train_airl(\"optimal\", 1400)"
   ],
   "id": "7d28b9f159d921ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training with different seeds and different nr of queries RLHF / IRLHF",
   "id": "56e77d98f7327ce8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T23:12:49.807677Z",
     "start_time": "2024-06-19T23:12:49.776367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "eval_mean_over_seeds = [] # [0]: lists of means for seed 0, num comp 250,500,600 respectively\n",
    "eval_std_over_seeds = [] \n",
    "\n",
    "def main_train(agent_name, path, expert_type, nr_demonstrations, random_prob=0, switch_prob=0):\n",
    "    seed_list = [34, 43, 52, 61, 70]\n",
    "    comparisons_list = [800]\n",
    "    \n",
    "    for i, seed in enumerate(seed_list): \n",
    "        # set seed\n",
    "        set_seed(seed)\n",
    "        \n",
    "        # start rlhf / irlhf \n",
    "        means = []\n",
    "        stds = []\n",
    "        eval_policy_mean = []\n",
    "        eval_policy_std = []\n",
    "        \n",
    "        for idx, num_comparisons in enumerate(comparisons_list):\n",
    "            if agent_name == \"rlhf\":\n",
    "                train_rlhf(num_comparisons)\n",
    "            if agent_name == \"irlhf\":\n",
    "                # Train airl under current seed\n",
    "                train_airl(expert_type=expert_type, nr_demonstrations=nr_demonstrations,\n",
    "                           random_prob=random_prob, switch_prob=switch_prob)\n",
    "                # Load trained AIRLAgent reward function\n",
    "                reward_net_airl = torch.load(\"airl_agent/reward_net.pt\")\n",
    "                # Pass reward_net_airl to irlhf\n",
    "                # Train irlhf (optimize reward function)\n",
    "                if num_comparisons == 10:\n",
    "                    train_irlhf(reward_net_airl, num_comparisons, num_it=8)\n",
    "                else:\n",
    "                    train_irlhf(reward_net_airl, num_comparisons)\n",
    "            \n",
    "            # Stats after learning reward function\n",
    "            rws, mean, std = get_reward_stats(path) \n",
    "            \n",
    "            means.append(mean)\n",
    "            stds.append(std)\n",
    "            \n",
    "            eval_mean = None\n",
    "            eval_std = None\n",
    "            \n",
    "            # Train agent using the learned reward, optimize policy\n",
    "            \n",
    "            if agent_name == \"rlhf\":\n",
    "                eval_mean, eval_std = train_with_learned_reward_and_evaluate(\n",
    "                    reward_path=\"rlhf_agent/reward_net.pt\",\n",
    "                    train_path=\"rlhf_agent/rlhf_agent_trained_with_learned_reward\",\n",
    "                    tensorboard_dir=f\"./ppo_rlhf_{env_name}_tensorboard_4/\",\n",
    "                    tb_log_name=f\"run_comparisons_{num_comparisons}_with_seed_{seed}_{env_name}\",\n",
    "                    wandb_project_name=\"rlhf\",\n",
    "                    wandb_save_path=f\"models/run_comparisons_{num_comparisons}_with_seed_{seed}\",\n",
    "                    config=config_rlhf\n",
    "                )\n",
    "            if agent_name == \"irlhf\":\n",
    "                eval_mean, eval_std = train_with_learned_reward_and_evaluate(\n",
    "                    reward_path=\"irlhf_agent/reward_net.pt\",\n",
    "                    train_path=\"irlhf_agent/irlhf_agent_trained_with_learned_reward\",\n",
    "                    tensorboard_dir=f\"./ppo_irlhf_{env_name}_tensorboard_4/\",\n",
    "                    tb_log_name=f\"run_irlhf_comparisons_{num_comparisons}_demonstrations_{nr_demonstrations}_with_seed_{seed}_with_{expert_type}_expert\",\n",
    "                    wandb_project_name=\"irlhf\",\n",
    "                    wandb_save_path=f\"models/run_comparisons_{num_comparisons}_with_seed_{seed}_irlhf\",\n",
    "                    config=config_rlhf\n",
    "                )\n",
    "                \n",
    "            eval_policy_mean.append(eval_mean)\n",
    "            eval_policy_std.append(eval_std)\n",
    "            \n",
    "            print(f\"number of comparisons = {num_comparisons}: mean={mean}, std={std}\")\n",
    "            print(f\"number of comparisons = {num_comparisons}: eval_mean={eval_mean}, eval_std={eval_std}\")\n",
    "        \n",
    "        # Append evaluation results for policy under true reward\n",
    "        eval_mean_over_seeds.append(eval_policy_mean)   # for every seed, all eval means for all comparisons are concatenated\n",
    "        eval_std_over_seeds.append(eval_policy_std)"
   ],
   "id": "ef1029fef7a06040",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T23:12:51.517578Z",
     "start_time": "2024-06-19T23:12:51.501906Z"
    }
   },
   "cell_type": "code",
   "source": "# main_train(\"rlhf\", \"./rlhf_agent/learner_rewards.pt\", expert_type=\"optimal\", nr_demonstrations=60)",
   "id": "8badcedc2672e530",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T09:30:31.485661Z",
     "start_time": "2024-06-20T09:30:31.475511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(eval_mean_over_seeds)\n",
    "print(eval_std_over_seeds)"
   ],
   "id": "2dbf624855d84c0e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.24, 12.08], [8.25, 10.04], [9.11, 116.55], [15.33, 8.34], [12.03, 13.08], [9.49, 12.62], [112.22, 11.47], [8.32, 11.56], [8.45, 79.61], [51.15, 8.3]]\n",
      "[[0.7889233169326407, 0.7959899496852959], [0.7664854858377946, 1.2158947322856533], [0.9580709785814411, 5.652211956393709], [2.533988950252151, 0.6959885056522126], [1.4659809002848572, 1.0552724766618333], [1.0535179163165664, 5.835717607972477], [4.3210646836167586, 0.8538735269347563], [0.7730459236035075, 1.275303885354389], [0.7262919523166975, 9.769232313749121], [7.397803728134452, 0.7810249675906655]]\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for demonstrations in [800]:\n",
    "    main_train(\"irlhf\", \"./irlhf_agent/learner_rewards.pt\", expert_type=\"suboptimal\", nr_demonstrations=demonstrations,\n",
    "                random_prob=0.10,\n",
    "                switch_prob=0.20)"
   ],
   "id": "c02575f6ea72e9ad",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
