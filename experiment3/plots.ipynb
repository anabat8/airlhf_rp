{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-10T19:58:09.493918Z",
     "start_time": "2024-06-10T19:58:05.536132Z"
    }
   },
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import torch \n",
    "import random\n",
    "\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from experiment3.RLHFAgent import RLHFAgent\n",
    "from experiment3.AIRLAgent import AIRLAgent\n",
    "from experiment3.Utils import Utils\n",
    "from experiment3.Environment import Environment"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize environment\n",
    "SEED = 42\n",
    "env = Environment(\"seals:seals/CartPole-v0\", SEED, num_envs=8)\n",
    "env.init_vec_env()"
   ],
   "id": "dffee0d2ba446207",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T19:58:11.088774Z",
     "start_time": "2024-06-10T19:58:11.072983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_rlhf(num_comparisons, exploration_frac=0.05, fragment_length=100):\n",
    "    # Initialize RLHFAgent\n",
    "    rlhfAgent = RLHFAgent(env_object=env)\n",
    "    rlhfAgent.init_gen_algo(policy_name=\"ppo\", ac_policy=MlpPolicy, env_object=env)\n",
    "    rlhfAgent.init_trajectory_gen(env_object=env, exploration_frac=exploration_frac)\n",
    "    \n",
    "    # Train RLHFAgent alone\n",
    "    rlhfAgent.train(save_path=pathlib.Path(\"rlhf_agent\"), env_object=env, total_human_comparisons=num_comparisons, total_timesteps=400_000,\n",
    "                    fragment_length=fragment_length)\n"
   ],
   "id": "373ad432019d8ff1",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T19:58:11.517174Z",
     "start_time": "2024-06-10T19:58:11.485535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_airl():\n",
    "    # Initialize AIRLAgent\n",
    "    airlAgent = AIRLAgent(env_object=env)\n",
    "    airlAgent.init_gen_algo(ac_policy=MlpPolicy, env_object=env)\n",
    "    \n",
    "    # Train AIRLAgent alone\n",
    "    # Train for >400k for 100% learned rewards, 280k for 70% learned rewards, 200k for 50% learned rewards, 80k for 20% learned rewards, 20k for 5% learned rewards\n",
    "    airlAgent.train(env_object=env, train_steps=20_000)"
   ],
   "id": "ee34b7d661c87f12",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T19:58:12.130917Z",
     "start_time": "2024-06-10T19:58:12.099737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_irlhf(reward_net_airl, num_comparisons, exploration_frac=0.05, fragment_length=100, num_it=60, initial_epoch_multiplier=4):\n",
    "    # Initialize RLHFAgent\n",
    "    rlhfAgent = RLHFAgent(env_object=env)\n",
    "    rlhfAgent.set_reward_from_airl(reward_net_airl, env_object=env)\n",
    "    # To only pass reward, remove path_to_algo parameter / or set to None\n",
    "    rlhfAgent.init_gen_algo(policy_name=\"ppo\", ac_policy=MlpPolicy, env_object=env, path_to_algo=\"airl_agent/gen_policy/model.zip\")\n",
    "    rlhfAgent.init_trajectory_gen(env_object=env, exploration_frac=exploration_frac)\n",
    "    \n",
    "    # Train RLHFAgent with reward initialized by AIRL\n",
    "    rlhfAgent.train(save_path=pathlib.Path(\"irlhf_agent\"), env_object=env, total_human_comparisons=num_comparisons, total_timesteps=400_000,\n",
    "                    fragment_length=fragment_length, num_it=num_it, initial_epoch_multiplier=initial_epoch_multiplier)\n",
    "    "
   ],
   "id": "425bfdb0a84a8496",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T19:58:12.843544Z",
     "start_time": "2024-06-10T19:58:12.827920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_reward_stats(path):\n",
    "    rewards = torch.load(path)\n",
    "    return rewards, np.mean(rewards), np.std(rewards)"
   ],
   "id": "c0ba932aa6bc8650",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T19:58:13.125603Z",
     "start_time": "2024-06-10T19:58:13.109974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.policies.base import NormalizeFeaturesExtractor\n",
    "\n",
    "\n",
    "def train_with_learned_reward_and_evaluate(reward_path, train_path, tensorboard_dir, tb_log_name, \n",
    "                                           wandb_project_name, wandb_save_path,\n",
    "                                           batch_size=64, lr=0.001, gamma=0.98, clip_range=0.2, n_epochs=20\n",
    "                                           ):\n",
    "    reward_net = torch.load(reward_path)\n",
    "    Utils.train_with_learned_reward(learned_reward=reward_net, save_path=train_path,  \n",
    "                                    ac_policy=MlpPolicy, tensorboard_dir=tensorboard_dir, tb_log_name=tb_log_name, \n",
    "                                    env_object=env, wandb_project_name=wandb_project_name, wandb_save_path=wandb_save_path,\n",
    "                                    batch_size=batch_size, lr=lr, gamma=gamma, clip_range=clip_range, n_epochs=n_epochs,\n",
    "                                    policy_kwargs=dict(\n",
    "                                        features_extractor_class=NormalizeFeaturesExtractor,\n",
    "                                        features_extractor_kwargs=dict(normalize_class=RunningNorm),\n",
    "                                    ))\n",
    "    eval_mean, eval_std = Utils.evaluate_trained_agent_with_learned_reward(load_path=train_path, venv=env.venv)\n",
    "    return eval_mean, eval_std"
   ],
   "id": "e98e428e75e819b2",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T19:58:13.926046Z",
     "start_time": "2024-06-10T19:58:13.894793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # wrap env in new seed\n",
    "    env.seed = seed \n",
    "    env.rng = np.random.default_rng(seed)\n",
    "    env.init_vec_env()\n",
    "    env.venv.seed(seed)"
   ],
   "id": "21d3cd0b107948f6",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training with different seeds and different demonstrations AIRL agent",
   "id": "798719038ed967c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T19:58:15.643631Z",
     "start_time": "2024-06-10T19:58:15.628072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main_train_airl():\n",
    "    rewards_over_seeds = [] \n",
    "    seed_list = [79]\n",
    "    # expert_demonstrations = []\n",
    "    \n",
    "    for i, seed in enumerate(seed_list): \n",
    "        # set seed\n",
    "        set_seed(seed)\n",
    "        \n",
    "        # start airl\n",
    "        means = []\n",
    "        stds = []\n",
    "        eval_policy_mean = []\n",
    "        eval_policy_std = []\n",
    "        \n",
    "        # Train\n",
    "        train_airl()\n",
    "        \n",
    "        rws, mean, std = get_reward_stats(\"./airl_agent/learner_rewards.pt\")\n",
    "            \n",
    "        means.append(mean)\n",
    "        stds.append(std)\n",
    "                    \n",
    "        eval_mean, eval_std = train_with_learned_reward_and_evaluate(\"airl_agent/reward_net.pt\", \"airl_agent/airl_agent_trained_with_learned_reward\",  \"./airl_cartpole_tensorboard\", f\"run_3_expert_with_{seed}\", \n",
    "         batch_size=64, lr=0.0005, gamma=0.95, clip_range=0.1, n_epochs=5,\n",
    "         wandb_project_name=\"airl\", wandb_save_path=f\"models/run_3_expert_with_{seed}\",)\n",
    "                \n",
    "        eval_policy_mean.append(eval_mean)\n",
    "        eval_policy_std.append(eval_std)\n",
    "            \n",
    "        # print(f\"number of expert demonstrations = {expert_dem}: mean={mean}, std={std}\")\n",
    "        # print(f\"number of expert demonstrations = {expert_dem}: eval_mean={eval_mean}, eval_std={eval_std}\")\n",
    "            \n",
    "        rewards_over_seeds.append(eval_policy_mean)"
   ],
   "id": "dc5a0590fd46a627",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "main_train_airl()",
   "id": "7d28b9f159d921ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training with different seeds and different nr of queries RLHF / IRLHF",
   "id": "56e77d98f7327ce8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T19:58:18.261229Z",
     "start_time": "2024-06-10T19:58:18.240774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main_train(agent_name, path):\n",
    "    rewards_over_seeds = [] # [0]: lists of means for seed 0, num comp 250,500,600 respectively\n",
    "    seed_list = [25, 34, 43, 52, 61, 70, 79]\n",
    "    comparisons_list = [10]\n",
    "    \n",
    "    for i, seed in enumerate(seed_list): \n",
    "        # set seed\n",
    "        set_seed(seed)\n",
    "        \n",
    "        # start rlhf / irlhf \n",
    "        means = []\n",
    "        stds = []\n",
    "        eval_policy_mean = []\n",
    "        eval_policy_std = []\n",
    "    \n",
    "        for idx, num_comparisons in enumerate(comparisons_list):\n",
    "            if agent_name == \"rlhf\":\n",
    "                train_rlhf(num_comparisons)\n",
    "            if agent_name == \"irlhf\":\n",
    "                # Train airl under current seed\n",
    "                train_airl()\n",
    "                # Load trained AIRLAgent reward function\n",
    "                reward_net_airl = torch.load(\"airl_agent/reward_net.pt\")\n",
    "                # Pass initialized reward function (can be 100%, 70%, 50%, 20%, 5%) to irlhf\n",
    "                # Train irlhf (optimize reward function)\n",
    "                if num_comparisons == 10:\n",
    "                    train_irlhf(reward_net_airl, num_comparisons, exploration_frac=0.05, fragment_length=100, num_it=8, initial_epoch_multiplier=4)\n",
    "                else:\n",
    "                    train_irlhf(reward_net_airl, num_comparisons, exploration_frac=0.05, fragment_length=100, num_it=60, initial_epoch_multiplier=4)\n",
    "                \n",
    "            rws, mean, std = get_reward_stats(path)\n",
    "            \n",
    "            means.append(mean)\n",
    "            stds.append(std)\n",
    "            \n",
    "            eval_mean = None\n",
    "            eval_std = None\n",
    "            \n",
    "            # Train agent using the learned reward, optimize policy\n",
    "            \n",
    "            if agent_name == \"rlhf\":\n",
    "                eval_mean, eval_std = train_with_learned_reward_and_evaluate(\n",
    "                    reward_path=\"rlhf_agent/reward_net.pt\",\n",
    "                    train_path=\"rlhf_agent/rlhf_agent_trained_with_learned_reward\",\n",
    "                    tensorboard_dir=\"./ppo_rlhf_cartpole_tensorboard/\",\n",
    "                    tb_log_name=f\"run5_comparisons_{num_comparisons}_with_seed_{seed}\",\n",
    "                    wandb_project_name=\"rlhf\",\n",
    "                    wandb_save_path=f\"models/run_comparisons_{num_comparisons}_with_seed_{seed}\"\n",
    "                )\n",
    "            if agent_name == \"irlhf\":\n",
    "                eval_mean, eval_std = train_with_learned_reward_and_evaluate(\n",
    "                    reward_path=\"irlhf_agent/reward_net.pt\",\n",
    "                    train_path=\"irlhf_agent/irlhf_agent_trained_with_learned_reward\",\n",
    "                    tensorboard_dir=\"./ppo_irlhf_cartpole_tensorboard/\",\n",
    "                    tb_log_name=f\"run9_comparisons_{num_comparisons}_with_seed_{seed}_irlhf_5%AIRLexpert_withPolicyInit\",\n",
    "                    wandb_project_name=\"irlhf\",\n",
    "                    wandb_save_path=f\"models/run_comparisons_{num_comparisons}_with_seed_{seed}_irlhf\"\n",
    "                )\n",
    "                \n",
    "            eval_policy_mean.append(eval_mean)\n",
    "            eval_policy_std.append(eval_std)\n",
    "            \n",
    "            print(f\"number of comparisons = {num_comparisons}: mean={mean}, std={std}\")\n",
    "            print(f\"number of comparisons = {num_comparisons}: eval_mean={eval_mean}, eval_std={eval_std}\")\n",
    "            \n",
    "        rewards_over_seeds.append(eval_policy_mean)"
   ],
   "id": "ef1029fef7a06040",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# main_train(\"rlhf\", \"./rlhf_agent/learner_rewards.pt\")\n",
    "main_train(\"irlhf\", \"./irlhf_agent/learner_rewards.pt\")"
   ],
   "id": "8badcedc2672e530",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "18027d7badfdf907"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
