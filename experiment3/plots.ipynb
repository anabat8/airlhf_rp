{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-10T13:56:42.465141Z",
     "start_time": "2024-06-10T13:56:39.652416Z"
    }
   },
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import random\n",
    "\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from experiment3.RLHFAgent import RLHFAgent\n",
    "from experiment3.AIRLAgent import AIRLAgent\n",
    "from experiment3.Utils import Utils\n",
    "from experiment3.Environment import Environment"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:56:42.496542Z",
     "start_time": "2024-06-10T13:56:42.465141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize environment\n",
    "SEED = 42\n",
    "env = Environment(\"seals:seals/CartPole-v0\", SEED, num_envs=8)\n",
    "env.init_vec_env()"
   ],
   "id": "dffee0d2ba446207",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ana\\PycharmProjects\\research_project\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:233: UserWarning: Starting from gymnasium v0.26, render modes are determined during the initialization of the environment.\n",
      "                We allow to pass a mode argument to maintain a backwards compatible VecEnv API, but the mode (rgb_array)\n",
      "                has to be the same as the environment render mode (None) which is not the case.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv at 0x2d02c035600>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:56:42.512267Z",
     "start_time": "2024-06-10T13:56:42.496542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_rlhf(num_comparisons, exploration_frac=0.05, fragment_length=100):\n",
    "    # Initialize RLHFAgent\n",
    "    rlhfAgent = RLHFAgent(env_object=env)\n",
    "    rlhfAgent.init_gen_algo(policy_name=\"PPO\", ac_policy=MlpPolicy, env_object=env)\n",
    "    rlhfAgent.init_trajectory_gen(env_object=env, exploration_frac=exploration_frac)\n",
    "    \n",
    "    # Train RLHFAgent alone\n",
    "    rlhfAgent.train(save_path=pathlib.Path(\"rlhf_agent\"), env_object=env, total_human_comparisons=num_comparisons, total_timesteps=400_000,\n",
    "                    fragment_length=fragment_length)\n"
   ],
   "id": "373ad432019d8ff1",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:56:42.528101Z",
     "start_time": "2024-06-10T13:56:42.512267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_airl():\n",
    "    # Initialize AIRLAgent\n",
    "    airlAgent = AIRLAgent(env_object=env)\n",
    "    airlAgent.init_gen_algo(ac_policy=MlpPolicy, env_object=env)\n",
    "    \n",
    "    # Train AIRLAgent alone\n",
    "    # Train for >400k for 100% expert, 280k for 70% expert, 200k for 50% expert, 80k for 20% expert, 20k for 5% expert\n",
    "    airlAgent.train(env_object=env, train_steps=20_000)"
   ],
   "id": "ee34b7d661c87f12",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:56:42.543864Z",
     "start_time": "2024-06-10T13:56:42.528101Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_irlhf(reward_net_irl, num_comparisons, exploration_frac=0.05, fragment_length=100, num_it=60, initial_epoch_multiplier=4):\n",
    "    # Initialize RLHFAgent\n",
    "    rlhfAgent = RLHFAgent(env_object=env)\n",
    "    rlhfAgent.set_reward_from_airl(reward_net_irl)\n",
    "    rlhfAgent.init_gen_algo(policy_name=\"PPO\", ac_policy=MlpPolicy, env_object=env)\n",
    "    rlhfAgent.init_trajectory_gen(env_object=env, exploration_frac=exploration_frac)\n",
    "    \n",
    "    # Train RLHFAgent with reward initialized by AIRL\n",
    "    rlhfAgent.train(save_path=pathlib.Path(\"irlhf_agent\"), env_object=env, total_human_comparisons=num_comparisons, total_timesteps=400_000,\n",
    "                    fragment_length=fragment_length, num_it=num_it, initial_epoch_multiplier=initial_epoch_multiplier)\n",
    "    "
   ],
   "id": "425bfdb0a84a8496",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:56:43.926572Z",
     "start_time": "2024-06-10T13:56:43.908069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_reward_stats(path):\n",
    "    rewards = th.load(path)\n",
    "    return rewards, np.mean(rewards), np.std(rewards)"
   ],
   "id": "c0ba932aa6bc8650",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:56:44.601009Z",
     "start_time": "2024-06-10T13:56:44.585385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.policies.base import NormalizeFeaturesExtractor\n",
    "\n",
    "\n",
    "def train_with_learned_reward_and_evaluate(reward_path, train_path, tensorboard_dir, tb_log_name, \n",
    "                                           wandb_project_name, wandb_save_path,\n",
    "                                           batch_size=64, lr=0.001, gamma=0.98, clip_range=0.2, n_epochs=20\n",
    "                                           ):\n",
    "    reward_net = th.load(reward_path)\n",
    "    Utils.train_with_learned_reward(learned_reward=reward_net, save_path=train_path,  \n",
    "                                    ac_policy=MlpPolicy, tensorboard_dir=tensorboard_dir, tb_log_name=tb_log_name, \n",
    "                                    env_object=env, wandb_project_name=wandb_project_name, wandb_save_path=wandb_save_path,\n",
    "                                    batch_size=batch_size, lr=lr, gamma=gamma, clip_range=clip_range, n_epochs=n_epochs,\n",
    "                                    policy_kwargs=dict(\n",
    "                                        features_extractor_class=NormalizeFeaturesExtractor,\n",
    "                                        features_extractor_kwargs=dict(normalize_class=RunningNorm),\n",
    "                                    ))\n",
    "    eval_mean, eval_std = Utils.evaluate_trained_agent_with_learned_reward(load_path=train_path, venv=env.venv)\n",
    "    return eval_mean, eval_std"
   ],
   "id": "e98e428e75e819b2",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:56:44.935043Z",
     "start_time": "2024-06-10T13:56:44.919413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    th.manual_seed(seed)\n",
    "    \n",
    "    if th.cuda.is_available():\n",
    "        th.cuda.manual_seed(seed)\n",
    "        th.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # wrap env in new seed\n",
    "    env.seed = seed \n",
    "    env.rng = np.random.default_rng(seed)\n",
    "    env.init_vec_env()\n",
    "    env.venv.seed(seed)"
   ],
   "id": "21d3cd0b107948f6",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training with different seeds and different demonstrations AIRL agent",
   "id": "798719038ed967c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:56:47.096471Z",
     "start_time": "2024-06-10T13:56:47.080920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main_train_airl():\n",
    "    rewards_over_seeds = [] \n",
    "    seed_list = [79]\n",
    "    # expert_demonstrations = []\n",
    "    \n",
    "    for i, seed in enumerate(seed_list): \n",
    "        # set seed\n",
    "        set_seed(seed)\n",
    "        \n",
    "        # start airl\n",
    "        means = []\n",
    "        stds = []\n",
    "        eval_policy_mean = []\n",
    "        eval_policy_std = []\n",
    "        \n",
    "        # Train\n",
    "        train_airl()\n",
    "        \n",
    "        rws, mean, std = get_reward_stats(\"./airl_agent/learner_rewards.pt\")\n",
    "            \n",
    "        means.append(mean)\n",
    "        stds.append(std)\n",
    "                    \n",
    "        eval_mean, eval_std = train_with_learned_reward_and_evaluate(\"airl_agent/reward_net.pt\", \"airl_agent/airl_agent_trained_with_learned_reward\",  \"./airl_cartpole_tensorboard\", f\"run_3_expert_with_{seed}\", \n",
    "         batch_size=64, lr=0.0005, gamma=0.95, clip_range=0.1, n_epochs=5,\n",
    "         wandb_project_name=\"airl\", wandb_save_path=f\"models/run_3_expert_with_{seed}\",)\n",
    "                \n",
    "        eval_policy_mean.append(eval_mean)\n",
    "        eval_policy_std.append(eval_std)\n",
    "            \n",
    "        # print(f\"number of expert demonstrations = {expert_dem}: mean={mean}, std={std}\")\n",
    "        # print(f\"number of expert demonstrations = {expert_dem}: eval_mean={eval_mean}, eval_std={eval_std}\")\n",
    "            \n",
    "        rewards_over_seeds.append(eval_policy_mean)"
   ],
   "id": "dc5a0590fd46a627",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "main_train_airl()",
   "id": "7d28b9f159d921ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training with different seeds and different nr of queries RLHF / IRLHF",
   "id": "56e77d98f7327ce8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:56:49.560784Z",
     "start_time": "2024-06-10T13:56:49.544097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main_train(agent_name, path):\n",
    "    rewards_over_seeds = [] # [0]: lists of means for seed 0, num comp 250,500,600 respectively\n",
    "    seed_list = [25, 34, 43, 52, 61, 70, 79]\n",
    "    comparisons_list = [700]\n",
    "    \n",
    "    for i, seed in enumerate(seed_list): \n",
    "        # set seed\n",
    "        set_seed(seed)\n",
    "        \n",
    "        # start rlhf / irlhf \n",
    "        means = []\n",
    "        stds = []\n",
    "        eval_policy_mean = []\n",
    "        eval_policy_std = []\n",
    "    \n",
    "        for idx, num_comparisons in enumerate(comparisons_list):\n",
    "            if agent_name == \"rlhf\":\n",
    "                train_rlhf(num_comparisons)\n",
    "            if agent_name == \"irlhf\":\n",
    "                # Train airl under current seed\n",
    "                train_airl()\n",
    "                # Load trained AIRLAgent reward function\n",
    "                reward_net_irl = th.load(\"airl_agent/reward_net.pt\")\n",
    "                # Pass initialized reward function (can be 100%, 70%, 50%, 20%, 5%) to irlhf\n",
    "                if num_comparisons == 10:\n",
    "                    train_irlhf(reward_net_irl, num_comparisons, exploration_frac=0.05, fragment_length=100, num_it=8, initial_epoch_multiplier=4)\n",
    "                else:\n",
    "                    train_irlhf(reward_net_irl, num_comparisons, exploration_frac=0.05, fragment_length=100, num_it=60, initial_epoch_multiplier=4)\n",
    "                \n",
    "            rws, mean, std = get_reward_stats(path)\n",
    "            \n",
    "            means.append(mean)\n",
    "            stds.append(std)\n",
    "            \n",
    "            eval_mean = None\n",
    "            eval_std = None\n",
    "            \n",
    "            if agent_name == \"rlhf\":\n",
    "                eval_mean, eval_std = train_with_learned_reward_and_evaluate(\n",
    "                    reward_path=\"rlhf_agent/reward_net.pt\",\n",
    "                    train_path=\"rlhf_agent/rlhf_agent_trained_with_learned_reward\",\n",
    "                    tensorboard_dir=\"./ppo_rlhf_cartpole_tensorboard/\",\n",
    "                    tb_log_name=f\"run5_comparisons_{num_comparisons}_with_seed_{seed}\",\n",
    "                    wandb_project_name=\"rlhf\",\n",
    "                    wandb_save_path=f\"models/run_comparisons_{num_comparisons}_with_seed_{seed}\"\n",
    "                )\n",
    "            if agent_name == \"irlhf\":\n",
    "                eval_mean, eval_std = train_with_learned_reward_and_evaluate(\n",
    "                    reward_path=\"irlhf_agent/reward_net.pt\",\n",
    "                    train_path=\"irlhf_agent/irlhf_agent_trained_with_learned_reward\",\n",
    "                    tensorboard_dir=\"./ppo_irlhf_cartpole_tensorboard/\",\n",
    "                    tb_log_name=f\"run7_comparisons_{num_comparisons}_with_seed_{seed}_irlhf_5%AIRLexpert\",\n",
    "                    wandb_project_name=\"irlhf\",\n",
    "                    wandb_save_path=f\"models/run_comparisons_{num_comparisons}_with_seed_{seed}_irlhf\"\n",
    "                )\n",
    "                \n",
    "            eval_policy_mean.append(eval_mean)\n",
    "            eval_policy_std.append(eval_std)\n",
    "            \n",
    "            print(f\"number of comparisons = {num_comparisons}: mean={mean}, std={std}\")\n",
    "            print(f\"number of comparisons = {num_comparisons}: eval_mean={eval_mean}, eval_std={eval_std}\")\n",
    "            \n",
    "        rewards_over_seeds.append(eval_policy_mean)"
   ],
   "id": "ef1029fef7a06040",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:57:20.429141Z",
     "start_time": "2024-06-10T13:56:50.265315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# main_train(\"rlhf\", \"./rlhf_agent/learner_rewards.pt\")\n",
    "main_train(\"irlhf\", \"./irlhf_agent/learner_rewards.pt\")"
   ],
   "id": "8badcedc2672e530",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ana\\PycharmProjects\\research_project\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:233: UserWarning: Starting from gymnasium v0.26, render modes are determined during the initialization of the environment.\n",
      "                We allow to pass a mode argument to maintain a backwards compatible VecEnv API, but the mode (rgb_array)\n",
      "                has to be the same as the environment render mode (None) which is not the case.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert stats:  {'n_traj': 64, 'return_min': 500.0, 'return_mean': 500.0, 'return_std': 0.0, 'return_max': 500.0, 'len_min': 500, 'len_mean': 500.0, 'len_std': 0.0, 'len_max': 500}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "round:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| raw/                        |          |\n",
      "|    gen/rollout/ep_len_mean  | 500      |\n",
      "|    gen/rollout/ep_rew_mean  | 32.6     |\n",
      "|    gen/time/fps             | 2924     |\n",
      "|    gen/time/iterations      | 1        |\n",
      "|    gen/time/time_elapsed    | 5        |\n",
      "|    gen/time/total_timesteps | 16384    |\n",
      "------------------------------------------\n",
      "--------------------------------------------------\n",
      "| raw/                                |          |\n",
      "|    disc/disc_acc                    | 0.509    |\n",
      "|    disc/disc_acc_expert             | 1        |\n",
      "|    disc/disc_acc_gen                | 0.0176   |\n",
      "|    disc/disc_entropy                | 0.635    |\n",
      "|    disc/disc_loss                   | 0.76     |\n",
      "|    disc/disc_proportion_expert_pred | 0.991    |\n",
      "|    disc/disc_proportion_expert_true | 0.5      |\n",
      "|    disc/global_step                 | 1        |\n",
      "|    disc/n_expert                    | 2.05e+03 |\n",
      "|    disc/n_generated                 | 2.05e+03 |\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "| raw/                                |          |\n",
      "|    disc/disc_acc                    | 0.507    |\n",
      "|    disc/disc_acc_expert             | 1        |\n",
      "|    disc/disc_acc_gen                | 0.0132   |\n",
      "|    disc/disc_entropy                | 0.637    |\n",
      "|    disc/disc_loss                   | 0.757    |\n",
      "|    disc/disc_proportion_expert_pred | 0.993    |\n",
      "|    disc/disc_proportion_expert_true | 0.5      |\n",
      "|    disc/global_step                 | 1        |\n",
      "|    disc/n_expert                    | 2.05e+03 |\n",
      "|    disc/n_generated                 | 2.05e+03 |\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "| raw/                                |          |\n",
      "|    disc/disc_acc                    | 0.509    |\n",
      "|    disc/disc_acc_expert             | 1        |\n",
      "|    disc/disc_acc_gen                | 0.0186   |\n",
      "|    disc/disc_entropy                | 0.639    |\n",
      "|    disc/disc_loss                   | 0.752    |\n",
      "|    disc/disc_proportion_expert_pred | 0.991    |\n",
      "|    disc/disc_proportion_expert_true | 0.5      |\n",
      "|    disc/global_step                 | 1        |\n",
      "|    disc/n_expert                    | 2.05e+03 |\n",
      "|    disc/n_generated                 | 2.05e+03 |\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "| raw/                                |          |\n",
      "|    disc/disc_acc                    | 0.515    |\n",
      "|    disc/disc_acc_expert             | 1        |\n",
      "|    disc/disc_acc_gen                | 0.0293   |\n",
      "|    disc/disc_entropy                | 0.642    |\n",
      "|    disc/disc_loss                   | 0.743    |\n",
      "|    disc/disc_proportion_expert_pred | 0.985    |\n",
      "|    disc/disc_proportion_expert_true | 0.5      |\n",
      "|    disc/global_step                 | 1        |\n",
      "|    disc/n_expert                    | 2.05e+03 |\n",
      "|    disc/n_generated                 | 2.05e+03 |\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "| raw/                                |          |\n",
      "|    disc/disc_acc                    | 0.51     |\n",
      "|    disc/disc_acc_expert             | 1        |\n",
      "|    disc/disc_acc_gen                | 0.02     |\n",
      "|    disc/disc_entropy                | 0.642    |\n",
      "|    disc/disc_loss                   | 0.746    |\n",
      "|    disc/disc_proportion_expert_pred | 0.99     |\n",
      "|    disc/disc_proportion_expert_true | 0.5      |\n",
      "|    disc/global_step                 | 1        |\n",
      "|    disc/n_expert                    | 2.05e+03 |\n",
      "|    disc/n_generated                 | 2.05e+03 |\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "| raw/                                |          |\n",
      "|    disc/disc_acc                    | 0.514    |\n",
      "|    disc/disc_acc_expert             | 1        |\n",
      "|    disc/disc_acc_gen                | 0.0283   |\n",
      "|    disc/disc_entropy                | 0.645    |\n",
      "|    disc/disc_loss                   | 0.737    |\n",
      "|    disc/disc_proportion_expert_pred | 0.986    |\n",
      "|    disc/disc_proportion_expert_true | 0.5      |\n",
      "|    disc/global_step                 | 1        |\n",
      "|    disc/n_expert                    | 2.05e+03 |\n",
      "|    disc/n_generated                 | 2.05e+03 |\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "| raw/                                |          |\n",
      "|    disc/disc_acc                    | 0.517    |\n",
      "|    disc/disc_acc_expert             | 1        |\n",
      "|    disc/disc_acc_gen                | 0.0332   |\n",
      "|    disc/disc_entropy                | 0.646    |\n",
      "|    disc/disc_loss                   | 0.735    |\n",
      "|    disc/disc_proportion_expert_pred | 0.983    |\n",
      "|    disc/disc_proportion_expert_true | 0.5      |\n",
      "|    disc/global_step                 | 1        |\n",
      "|    disc/n_expert                    | 2.05e+03 |\n",
      "|    disc/n_generated                 | 2.05e+03 |\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "| raw/                                |          |\n",
      "|    disc/disc_acc                    | 0.515    |\n",
      "|    disc/disc_acc_expert             | 1        |\n",
      "|    disc/disc_acc_gen                | 0.0303   |\n",
      "|    disc/disc_entropy                | 0.648    |\n",
      "|    disc/disc_loss                   | 0.731    |\n",
      "|    disc/disc_proportion_expert_pred | 0.985    |\n",
      "|    disc/disc_proportion_expert_true | 0.5      |\n",
      "|    disc/global_step                 | 1        |\n",
      "|    disc/n_expert                    | 2.05e+03 |\n",
      "|    disc/n_generated                 | 2.05e+03 |\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "| raw/                                |          |\n",
      "|    disc/disc_acc                    | 0.515    |\n",
      "|    disc/disc_acc_expert             | 1        |\n",
      "|    disc/disc_acc_gen                | 0.0303   |\n",
      "|    disc/disc_entropy                | 0.649    |\n",
      "|    disc/disc_loss                   | 0.729    |\n",
      "|    disc/disc_proportion_expert_pred | 0.985    |\n",
      "|    disc/disc_proportion_expert_true | 0.5      |\n",
      "|    disc/global_step                 | 1        |\n",
      "|    disc/n_expert                    | 2.05e+03 |\n",
      "|    disc/n_generated                 | 2.05e+03 |\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "| raw/                                |          |\n",
      "|    disc/disc_acc                    | 0.52     |\n",
      "|    disc/disc_acc_expert             | 1        |\n",
      "|    disc/disc_acc_gen                | 0.0405   |\n",
      "|    disc/disc_entropy                | 0.65     |\n",
      "|    disc/disc_loss                   | 0.727    |\n",
      "|    disc/disc_proportion_expert_pred | 0.98     |\n",
      "|    disc/disc_proportion_expert_true | 0.5      |\n",
      "|    disc/global_step                 | 1        |\n",
      "|    disc/n_expert                    | 2.05e+03 |\n",
      "|    disc/n_generated                 | 2.05e+03 |\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "| raw/                                |          |\n",
      "|    disc/disc_acc                    | 0.521    |\n",
      "|    disc/disc_acc_expert             | 1        |\n",
      "|    disc/disc_acc_gen                | 0.0425   |\n",
      "|    disc/disc_entropy                | 0.652    |\n",
      "|    disc/disc_loss                   | 0.72     |\n",
      "|    disc/disc_proportion_expert_pred | 0.979    |\n",
      "|    disc/disc_proportion_expert_true | 0.5      |\n",
      "|    disc/global_step                 | 1        |\n",
      "|    disc/n_expert                    | 2.05e+03 |\n",
      "|    disc/n_generated                 | 2.05e+03 |\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "| raw/                                |          |\n",
      "|    disc/disc_acc                    | 0.525    |\n",
      "|    disc/disc_acc_expert             | 1        |\n",
      "|    disc/disc_acc_gen                | 0.0498   |\n",
      "|    disc/disc_entropy                | 0.654    |\n",
      "|    disc/disc_loss                   | 0.717    |\n",
      "|    disc/disc_proportion_expert_pred | 0.975    |\n",
      "|    disc/disc_proportion_expert_true | 0.5      |\n",
      "|    disc/global_step                 | 1        |\n",
      "|    disc/n_expert                    | 2.05e+03 |\n",
      "|    disc/n_generated                 | 2.05e+03 |\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "| raw/                                |          |\n",
      "|    disc/disc_acc                    | 0.522    |\n",
      "|    disc/disc_acc_expert             | 1        |\n",
      "|    disc/disc_acc_gen                | 0.0444   |\n",
      "|    disc/disc_entropy                | 0.655    |\n",
      "|    disc/disc_loss                   | 0.716    |\n",
      "|    disc/disc_proportion_expert_pred | 0.978    |\n",
      "|    disc/disc_proportion_expert_true | 0.5      |\n",
      "|    disc/global_step                 | 1        |\n",
      "|    disc/n_expert                    | 2.05e+03 |\n",
      "|    disc/n_generated                 | 2.05e+03 |\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "| raw/                                |          |\n",
      "|    disc/disc_acc                    | 0.528    |\n",
      "|    disc/disc_acc_expert             | 1        |\n",
      "|    disc/disc_acc_gen                | 0.0552   |\n",
      "|    disc/disc_entropy                | 0.656    |\n",
      "|    disc/disc_loss                   | 0.711    |\n",
      "|    disc/disc_proportion_expert_pred | 0.972    |\n",
      "|    disc/disc_proportion_expert_true | 0.5      |\n",
      "|    disc/global_step                 | 1        |\n",
      "|    disc/n_expert                    | 2.05e+03 |\n",
      "|    disc/n_generated                 | 2.05e+03 |\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "| raw/                                |          |\n",
      "|    disc/disc_acc                    | 0.53     |\n",
      "|    disc/disc_acc_expert             | 1        |\n",
      "|    disc/disc_acc_gen                | 0.0605   |\n",
      "|    disc/disc_entropy                | 0.656    |\n",
      "|    disc/disc_loss                   | 0.713    |\n",
      "|    disc/disc_proportion_expert_pred | 0.97     |\n",
      "|    disc/disc_proportion_expert_true | 0.5      |\n",
      "|    disc/global_step                 | 1        |\n",
      "|    disc/n_expert                    | 2.05e+03 |\n",
      "|    disc/n_generated                 | 2.05e+03 |\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "| raw/                                |          |\n",
      "|    disc/disc_acc                    | 0.533    |\n",
      "|    disc/disc_acc_expert             | 1        |\n",
      "|    disc/disc_acc_gen                | 0.0654   |\n",
      "|    disc/disc_entropy                | 0.658    |\n",
      "|    disc/disc_loss                   | 0.709    |\n",
      "|    disc/disc_proportion_expert_pred | 0.967    |\n",
      "|    disc/disc_proportion_expert_true | 0.5      |\n",
      "|    disc/global_step                 | 1        |\n",
      "|    disc/n_expert                    | 2.05e+03 |\n",
      "|    disc/n_generated                 | 2.05e+03 |\n",
      "--------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                               |           |\n",
      "|    disc/disc_acc                    | 0.518     |\n",
      "|    disc/disc_acc_expert             | 1         |\n",
      "|    disc/disc_acc_gen                | 0.0362    |\n",
      "|    disc/disc_entropy                | 0.648     |\n",
      "|    disc/disc_loss                   | 0.731     |\n",
      "|    disc/disc_proportion_expert_pred | 0.982     |\n",
      "|    disc/disc_proportion_expert_true | 0.5       |\n",
      "|    disc/global_step                 | 1         |\n",
      "|    disc/n_expert                    | 2.05e+03  |\n",
      "|    disc/n_generated                 | 2.05e+03  |\n",
      "|    gen/rollout/ep_len_mean          | 500       |\n",
      "|    gen/rollout/ep_rew_mean          | 32.6      |\n",
      "|    gen/time/fps                     | 2.92e+03  |\n",
      "|    gen/time/iterations              | 1         |\n",
      "|    gen/time/time_elapsed            | 5         |\n",
      "|    gen/time/total_timesteps         | 1.64e+04  |\n",
      "|    gen/train/approx_kl              | 0.00131   |\n",
      "|    gen/train/clip_fraction          | 0.0371    |\n",
      "|    gen/train/clip_range             | 0.1       |\n",
      "|    gen/train/entropy_loss           | -0.692    |\n",
      "|    gen/train/explained_variance     | -0.00826  |\n",
      "|    gen/train/learning_rate          | 0.0005    |\n",
      "|    gen/train/loss                   | 0.407     |\n",
      "|    gen/train/n_updates              | 5         |\n",
      "|    gen/train/policy_gradient_loss   | -0.000674 |\n",
      "|    gen/train/value_loss             | 29        |\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "round: 100%|██████████| 1/1 [00:11<00:00, 11.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards before training: 206.74 +/- 186.23751609168332\n",
      "Rewards after training: 8.31 +/- 0.7706490770772388\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'AIRL' object has no attribute 'reward_net'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# main_train(\"rlhf\", \"./rlhf_agent/learner_rewards.pt\")\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[43mmain_train\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mirlhf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m./irlhf_agent/learner_rewards.pt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[10], line 21\u001B[0m, in \u001B[0;36mmain_train\u001B[1;34m(agent_name, path)\u001B[0m\n\u001B[0;32m     18\u001B[0m     train_rlhf(num_comparisons)\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m agent_name \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mirlhf\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m     20\u001B[0m     \u001B[38;5;66;03m# Train airl under current seed\u001B[39;00m\n\u001B[1;32m---> 21\u001B[0m     \u001B[43mtrain_airl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     22\u001B[0m     \u001B[38;5;66;03m# Load trained AIRLAgent reward function\u001B[39;00m\n\u001B[0;32m     23\u001B[0m     reward_net_irl \u001B[38;5;241m=\u001B[39m th\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mairl_agent/reward_net.pt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[4], line 8\u001B[0m, in \u001B[0;36mtrain_airl\u001B[1;34m()\u001B[0m\n\u001B[0;32m      4\u001B[0m airlAgent\u001B[38;5;241m.\u001B[39minit_gen_algo(ac_policy\u001B[38;5;241m=\u001B[39mMlpPolicy, env_object\u001B[38;5;241m=\u001B[39menv)\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# Train AIRLAgent alone\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# Train for >400k for 100% expert, 280k for 70% expert, 200k for 50% expert, 80k for 20% expert, 20k for 5% expert\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m \u001B[43mairlAgent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv_object\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m20_000\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\Year3\\Research Project\\research_project\\experiment3\\AIRLAgent.py:89\u001B[0m, in \u001B[0;36mAIRLAgent.train\u001B[1;34m(self, env_object, train_steps, batch_size, buffer_cap, n_disc_updates_per_round)\u001B[0m\n\u001B[0;32m     81\u001B[0m \u001B[38;5;28mprint\u001B[39m(\n\u001B[0;32m     82\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRewards after training:\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     83\u001B[0m     np\u001B[38;5;241m.\u001B[39mmean(learner_rewards_after_training),\n\u001B[0;32m     84\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m+/-\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     85\u001B[0m     np\u001B[38;5;241m.\u001B[39mstd(learner_rewards_after_training),\n\u001B[0;32m     86\u001B[0m )\n\u001B[0;32m     88\u001B[0m \u001B[38;5;66;03m# Save model\u001B[39;00m\n\u001B[1;32m---> 89\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43msave_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpathlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPath\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mairl_agent\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearner_rewards\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlearner_rewards_after_training\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\Year3\\Research Project\\research_project\\experiment3\\AIRLAgent.py:96\u001B[0m, in \u001B[0;36mAIRLAgent.save\u001B[1;34m(self, save_path, learner_rewards)\u001B[0m\n\u001B[0;32m     94\u001B[0m save_path\u001B[38;5;241m.\u001B[39mmkdir(parents\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, exist_ok\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     95\u001B[0m th\u001B[38;5;241m.\u001B[39msave(learner_rewards, save_path \u001B[38;5;241m/\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlearner_rewards.pt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 96\u001B[0m th\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mairl_trainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreward_net\u001B[49m, save_path \u001B[38;5;241m/\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreward_net.pt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     97\u001B[0m th\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mairl_trainer\u001B[38;5;241m.\u001B[39mreward_train, save_path \u001B[38;5;241m/\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreward_train.pt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     98\u001B[0m th\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mairl_trainer\u001B[38;5;241m.\u001B[39mreward_test, save_path \u001B[38;5;241m/\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreward_test.pt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'AIRL' object has no attribute 'reward_net'"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4c9015d5aa964724",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
