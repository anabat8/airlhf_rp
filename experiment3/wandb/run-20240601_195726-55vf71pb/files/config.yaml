wandb_version: 1

policy_type:
  desc: null
  value: stable_baselines3.common.policies.ActorCriticPolicy
total_timesteps:
  desc: null
  value: 400000
env_name:
  desc: null
  value: CartPole-v0
_wandb:
  desc: null
  value:
    python_version: 3.10.11
    cli_version: 0.17.0
    framework: torch
    is_jupyter_run: true
    is_kaggle_kernel: false
    start_time: 1717264646
    t:
      1:
      - 1
      - 2
      - 3
      - 49
      - 55
      2:
      - 1
      - 2
      - 3
      - 49
      - 55
      3:
      - 1
      - 16
      - 22
      - 23
      - 35
      4: 3.10.11
      5: 0.17.0
      8:
      - 1
      - 3
      - 5
      13: windows-amd64
algo:
  desc: null
  value: PPO
policy_class:
  desc: null
  value: <class 'stable_baselines3.common.policies.ActorCriticPolicy'>
device:
  desc: null
  value: cpu
verbose:
  desc: null
  value: 1
policy_kwargs:
  desc: null
  value: '{''features_extractor_class'': <class ''imitation.policies.base.NormalizeFeaturesExtractor''>,
    ''features_extractor_kwargs'': {''normalize_class'': <class ''imitation.util.networks.RunningNorm''>}}'
num_timesteps:
  desc: null
  value: 0
_total_timesteps:
  desc: null
  value: 400000
_num_timesteps_at_start:
  desc: null
  value: 0
seed:
  desc: null
  value: 42
action_noise:
  desc: null
  value: None
start_time:
  desc: null
  value: 1717264647648708500
learning_rate:
  desc: null
  value: 0.0005
tensorboard_log:
  desc: null
  value: ./airl_cartpole_tensorboard
_last_obs:
  desc: null
  value: "[[ 0.0273956  -0.00611216  0.03585979  0.0197368 ]\n [ 0.01522993 -0.04562247\
    \ -0.04799704  0.03392126]\n [-0.03774345 -0.02418869 -0.00942293  0.0469184 ]\n\
    \ [ 0.00731307  0.00284911  0.02636502  0.03116928]\n [ 0.04056043 -0.04227732\
    \ -0.02274302  0.01218505]\n [ 0.0241802   0.02536688 -0.00348195 -0.0396275 ]\n\
    \ [-0.01123001  0.00957735  0.00138005  0.01944919]\n [-0.01371458  0.00932173\
    \ -0.01080498  0.01236993]]"
_last_episode_starts:
  desc: null
  value: '[ True  True  True  True  True  True  True  True]'
_last_original_obs:
  desc: null
  value: None
_episode_num:
  desc: null
  value: 0
use_sde:
  desc: null
  value: 'False'
sde_sample_freq:
  desc: null
  value: -1
_current_progress_remaining:
  desc: null
  value: 1.0
_stats_window_size:
  desc: null
  value: 100
ep_info_buffer:
  desc: null
  value: deque([], maxlen=100)
ep_success_buffer:
  desc: null
  value: deque([], maxlen=100)
_n_updates:
  desc: null
  value: 0
_custom_logger:
  desc: null
  value: 'False'
env:
  desc: null
  value: <imitation.rewards.reward_wrapper.RewardVecEnvWrapper object at 0x000001EED45269E0>
_vec_normalize_env:
  desc: null
  value: None
observation_space:
  desc: null
  value: Box([-3.4028235e+38 -3.4028235e+38 -3.1415927e+00 -3.4028235e+38], [3.4028235e+38
    3.4028235e+38 3.1415927e+00 3.4028235e+38], (4,), float32)
action_space:
  desc: null
  value: Discrete(2)
n_envs:
  desc: null
  value: 8
n_steps:
  desc: null
  value: 32
gamma:
  desc: null
  value: 0.95
gae_lambda:
  desc: null
  value: 0.8
ent_coef:
  desc: null
  value: 0.0
vf_coef:
  desc: null
  value: 0.1
max_grad_norm:
  desc: null
  value: 0.5
rollout_buffer_class:
  desc: null
  value: <class 'stable_baselines3.common.buffers.RolloutBuffer'>
rollout_buffer_kwargs:
  desc: null
  value: '{}'
batch_size:
  desc: null
  value: 64
n_epochs:
  desc: null
  value: 5
clip_range:
  desc: null
  value: <function get_schedule_fn.<locals>.<lambda> at 0x000001EEA8BA6170>
clip_range_vf:
  desc: null
  value: None
normalize_advantage:
  desc: null
  value: 'True'
target_kl:
  desc: null
  value: None
lr_schedule:
  desc: null
  value: <function get_schedule_fn.<locals>.<lambda> at 0x000001EEA4581EA0>
rollout_buffer:
  desc: null
  value: <stable_baselines3.common.buffers.RolloutBuffer object at 0x000001EEA4587C10>
policy:
  desc: null
  value: "ActorCriticPolicy(\n  (features_extractor): NormalizeFeaturesExtractor(\n\
    \    (flatten): Flatten(start_dim=1, end_dim=-1)\n    (normalize): RunningNorm()\n\
    \  )\n  (pi_features_extractor): NormalizeFeaturesExtractor(\n    (flatten): Flatten(start_dim=1,\
    \ end_dim=-1)\n    (normalize): RunningNorm()\n  )\n  (vf_features_extractor):\
    \ NormalizeFeaturesExtractor(\n    (flatten): Flatten(start_dim=1, end_dim=-1)\n\
    \    (normalize): RunningNorm()\n  )\n  (mlp_extractor): MlpExtractor(\n    (policy_net):\
    \ Sequential(\n      (0): Linear(in_features=4, out_features=64, bias=True)\n\
    \      (1): Tanh()\n      (2): Linear(in_features=64, out_features=64, bias=True)\n\
    \      (3): Tanh()\n    )\n    (value_net): Sequential(\n      (0): Linear(in_features=4,\
    \ out_features=64, bias=True)\n      (1): Tanh()\n      (2): Linear(in_features=64,\
    \ out_features=64, bias=True)\n      (3): Tanh()\n    )\n  )\n  (action_net):\
    \ Linear(in_features=64, out_features=2, bias=True)\n  (value_net): Linear(in_features=64,\
    \ out_features=1, bias=True)\n)"
_logger:
  desc: null
  value: <stable_baselines3.common.logger.Logger object at 0x000001EEA951FDF0>
